{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29398,"status":"ok","timestamp":1671892293167,"user":{"displayName":"ibuki","userId":"11094945753450125699"},"user_tz":-540},"id":"WFq_mc39ugAv","outputId":"319e748c-8b4a-4b49-e493-511853a84ab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3d5BZvbEucwd","executionInfo":{"status":"ok","timestamp":1671892937694,"user_tz":-540,"elapsed":40682,"user":{"displayName":"ibuki","userId":"11094945753450125699"}},"outputId":"1f0e1115-3c23-4a2a-b7c8-dd0a644f22d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0件取得しました。\n"]}],"source":["import pandas as pd\n","import requests\n","import json\n","import re\n","import ast\n","import datetime\n","import time\n","\n","def create_url(QUERY, MAX_RESULTS):\n","    # クエリ条件：指定のワードを含む、リツイートを除く、botと思われるユーザーのツイートを除く\n","    query = QUERY\n","    tweet_fields = \"tweet.fields=author_id,id,text,created_at\"\n","    max_results = MAX_RESULTS\n","    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}&{}\".format(\n","        query, tweet_fields, max_results\n","    )\n","    return url\n","\n","def create_headers(bearer_token):\n","    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n","    return headers\n","\n","def connect_to_endpoint(url, headers):\n","    response = requests.request(\"GET\", url, headers=headers)\n","#     print('status code:', str(response.status_code))\n","    if response.status_code != 200:\n","        raise Exception(response.status_code, response.text)\n","    return response.json()\n","\n","def get_tweet(BEARER_TOKEN, MAX_RESULTS, QUERY):\n","    bearer_token = BEARER_TOKEN\n","    url = create_url(QUERY, MAX_RESULTS)\n","    headers = create_headers(bearer_token)\n","    json_response = connect_to_endpoint(url, headers)\n","    json_dumps = json.dumps(json_response, indent=4, sort_keys=True)\n","    return ast.literal_eval(re.sub('\\\\n\\s+', '', json_dumps))\n","\n","def utc_to_jst(timestamp_utc):\n","    datetime_utc = datetime.datetime.strptime(timestamp_utc + \"+0000\", \"%Y-%m-%d %H:%M:%S.%f%z\")\n","    datetime_jst = datetime_utc.astimezone(datetime.timezone(datetime.timedelta(hours=+9)))\n","    timestamp_jst = datetime.datetime.strftime(datetime_jst, '%Y-%m-%d %H:%M:%S')\n","    return timestamp_jst\n","\n","def shape_data(data):\n","    for i, d in enumerate(data):\n","        # URLの削除\n","        data[i]['text'] = re.sub('[ 　]https://t\\.co/[a-zA-Z0-9]+', '', d['text'])\n","        # ユーザー名の削除\n","        data[i]['text'] = re.sub('[ 　]?@[a-zA-Z0-9_]+[ 　]', '', d['text'])\n","        # 絵文字の除去\n","        data[i]['text'] = d['text'].encode('cp932',errors='ignore').decode('cp932')\n","#         # ハッシュタグの削除\n","#         data[i]['text'] = re.sub('#.+ ', '', d['text'])\n","        # 全角スペース、タブ、改行を削除\n","        data[i]['text'] = re.sub(r\"[\\u3000\\t\\n]\", \"\", d['text'])\n","        # 日付時刻の変換（UTCからJST）\n","        data[i]['created_at'] = utc_to_jst(d['created_at'].replace('T', ' ')[:-1])\n","    return data\n","\n","BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAALLJkgEAAAAAglz5cQAEboOqJo1p10YnYiFJ34s%3DGhZA8au9oQvIxQNpqL5F1eRLNygkLpYN1WG7kkVCL72DAS8wwS\"\n","MAX_RESULTS = \"max_results=100\" # A number between 10 and 100.\n","\n","TARGET_WORDS = [\n","    \"クリスマス\",\n","    \"Xmas\"\n","]\n","QUERY_CONDITIONS = [\n","    \" -is:retweet -(from:HOGE OR from:FUGA)\",\n","    \" -is:retweet -(from:FOO OR from:BAR)\"\n","]\n","\n","df = pd.DataFrame()\n","iterator, request_iterator = 0, 0\n","\n","# クエリのlistが終わるまでAPIを叩く\n","r_count = 0\n","for target_word, query_ in zip(TARGET_WORDS, QUERY_CONDITIONS):\n","    next_token = ''\n","    break_flag = False\n","    # 次ページがなくなるまで次ページのクエリを取得\n","    while True:\n","        try:\n","            data['meta']['next_token']\n","        except KeyError: # 次ページがない(next_tokenがない)場合はループを抜ける\n","            del data\n","            break_flag = True\n","        except NameError: # TARGET_WORDS内の各要素で初めてAPIを取得するとき\n","            query = query_\n","        else: # 2ページめ以降の処理\n","            next_token = data['meta']['next_token']\n","            query = query_ + '&next_token=' + next_token\n","        finally:\n","            if break_flag == True: break\n","            QUERY = '{}{}'.format(target_word, query)\n","            try:\n","                data = get_tweet(BEARER_TOKEN, MAX_RESULTS, QUERY)\n","                temp_df = pd.DataFrame(shape_data(data['data']))\n","                temp_df[target_word] = True\n","                df = pd.concat([df, temp_df])\n","                iterator += data['meta']['result_count']\n","            except:\n","                pass\n","            \n","            request_iterator += 1\n","            if request_iterator >= 180: # 180requestを超えたら止める\n","                r_count += 1\n","                if r_count > 0: break\n","                print('180リクエストを超えるため、15分間停止します...')\n","                time.sleep(15.01*60) # 15分間（余裕をみてプラス1秒弱）中断\n","                request_iterator = 0\n","                # df_tmp = df.copy()\n","                # df_tmp.reset_index(drop=True, inplace=True)\n","                # df_tmp.to_pickle('/content/drive/MyDrive/プログラミング/AcademiX/勉強会/Qiita1224/raw_tweetlog_tmp.pkl')\n","\n","print(str(iterator) + '件取得しました。')\n","df.reset_index(drop=True, inplace=True)\n","df.to_pickle('/content/drive/MyDrive/プログラミング/AcademiX/勉強会/Qiita1224/raw_tweetlog_tmp.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUAteOh-pklz"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOhQl4DlFURMUmae2E8v2NY"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}